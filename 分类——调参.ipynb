{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f451a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "import random\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from bayes_opt import BayesianOptimization\n",
    "import optunity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2becda29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_more(n):\n",
    "    mt_x_1 = np.random.rand(n,5)\n",
    "    mt_x_integar = np.random.choice([18,20,22,23,24,26], size = n,p=[0.1,0.2,0.3,0.1,0.1,0.2]).reshape(-1,1)\n",
    "    mt_x_b1 = np.random.binomial(1, 0.4,size = n).reshape(-1,1)\n",
    "    mt_x_tri1 = np.random.binomial(1, 0.5,size = n).reshape(-1,1)\n",
    "    mt_x_tri2 = np.random.binomial(1, 0.8,size = n).reshape(-1,1)\n",
    "    for i in range(n):\n",
    "        if mt_x_tri1[i] + mt_x_tri2[i] > 1:\n",
    "            mt_x_tri2[i] = 0\n",
    "    mt_x = np.concatenate((mt_x_1, mt_x_integar,mt_x_b1,mt_x_tri1,mt_x_tri2),axis = 1)\n",
    "    mt_y = np.zeros(n).reshape(n,1)\n",
    "    mt_yp = np.zeros(n).reshape(n,1)   \n",
    "    for i in range(n):\n",
    "        mt_yp[i]=math.exp(0.6*mt_x[i,0]-8*mt_x[i,1]-4*mt_x[i,2]+0.4*mt_x[i,5]-7*mt_x[i,6]+2*mt_x[i,7]-4*mt_x[i,8])/(1+math.exp(0.6*mt_x[i,0]-8*mt_x[i,1]-4*mt_x[i,2]+0.4*mt_x[i,5]-7*mt_x[i,6]+2*mt_x[i,7]-4*mt_x[i,8]))\n",
    "        mt_y[i]= np.random.binomial(1, mt_yp[i],size = 1)\n",
    "#         print(mt_yp[i])\n",
    "    mt_data = np.concatenate((mt_x, mt_y),axis=1)\n",
    "    mt_data = pd.DataFrame(mt_data)\n",
    "    return mt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bd12c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_box_function(n_estimators, max_depth):  # min_samples_split, max_features,\n",
    "    xg_bayes = XGBClassifier(n_estimators=int(n_estimators),\n",
    "#                                 min_samples_split=int(min_samples_split),\n",
    "#                                 max_features=min(max_features, 0.999),  # float\n",
    "                                max_depth=int(max_depth),\n",
    "                                random_state=2,\n",
    "                             use_label_encoder=False).fit(X_train, y_train)\n",
    "    xg_bayes_predict = xg_bayes.predict_proba(X_test)\n",
    "    res = 1 / log_loss(y_test, xg_bayes_predict)\n",
    "    return res\n",
    "pbounds= {'n_estimators': (10, 300),\n",
    "#          'min_samples_split': (2, 25),\n",
    "#          'max_features': (0.1, 0.999),\n",
    "         'max_depth': (5, 30)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fae0fe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:13:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "|   iter    |  target   | max_depth | n_esti... |\n",
      "-------------------------------------------------\n",
      "[00:13:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 2.412   \u001b[0m | \u001b[0m 15.43   \u001b[0m | \u001b[0m 218.9   \u001b[0m |\n",
      "[00:13:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 2.517   \u001b[0m | \u001b[95m 5.003   \u001b[0m | \u001b[95m 97.68   \u001b[0m |\n",
      "[00:13:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 2.218   \u001b[0m | \u001b[0m 8.669   \u001b[0m | \u001b[0m 36.78   \u001b[0m |\n",
      "[00:13:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 2.5     \u001b[0m | \u001b[0m 9.657   \u001b[0m | \u001b[0m 110.2   \u001b[0m |\n",
      "[00:13:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 2.462   \u001b[0m | \u001b[0m 14.92   \u001b[0m | \u001b[0m 166.3   \u001b[0m |\n",
      "[00:13:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 2.5     \u001b[0m | \u001b[0m 6.298   \u001b[0m | \u001b[0m 98.38   \u001b[0m |\n",
      "[00:13:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 2.49    \u001b[0m | \u001b[0m 5.557   \u001b[0m | \u001b[0m 79.93   \u001b[0m |\n",
      "[00:13:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 2.453   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 129.6   \u001b[0m |\n",
      "[00:13:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 2.484   \u001b[0m | \u001b[0m 29.29   \u001b[0m | \u001b[0m 120.6   \u001b[0m |\n",
      "[00:13:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 2.455   \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 143.8   \u001b[0m |\n",
      "[00:13:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 2.323   \u001b[0m | \u001b[0m 28.46   \u001b[0m | \u001b[0m 299.8   \u001b[0m |\n",
      "[00:13:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 2.431   \u001b[0m | \u001b[0m 29.86   \u001b[0m | \u001b[0m 184.9   \u001b[0m |\n",
      "[00:13:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 2.476   \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 78.11   \u001b[0m |\n",
      "[00:13:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 2.51    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 101.8   \u001b[0m |\n",
      "[00:13:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 2.359   \u001b[0m | \u001b[0m 5.381   \u001b[0m | \u001b[0m 257.1   \u001b[0m |\n",
      "[00:13:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 2.441   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 187.3   \u001b[0m |\n",
      "[00:13:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 2.448   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 151.0   \u001b[0m |\n",
      "[00:13:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 1.832   \u001b[0m | \u001b[0m 29.39   \u001b[0m | \u001b[0m 10.37   \u001b[0m |\n",
      "[00:13:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 2.366   \u001b[0m | \u001b[0m 29.9    \u001b[0m | \u001b[0m 246.1   \u001b[0m |\n",
      "[00:13:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 2.323   \u001b[0m | \u001b[0m 5.097   \u001b[0m | \u001b[0m 299.6   \u001b[0m |\n",
      "[00:13:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 2.425   \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 208.8   \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:13:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 2.336   \u001b[0m | \u001b[0m 5.149   \u001b[0m | \u001b[0m 62.59   \u001b[0m |\n",
      "[00:13:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 2.46    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 162.9   \u001b[0m |\n",
      "[00:13:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 2.366   \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 272.6   \u001b[0m |\n",
      "[00:13:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 2.5     \u001b[0m | \u001b[0m 29.73   \u001b[0m | \u001b[0m 89.8    \u001b[0m |\n",
      "[00:13:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 2.416   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 205.9   \u001b[0m |\n",
      "[00:13:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 2.5     \u001b[0m | \u001b[0m 5.253   \u001b[0m | \u001b[0m 110.4   \u001b[0m |\n",
      "[00:13:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 2.5     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 89.14   \u001b[0m |\n",
      "[00:13:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 2.475   \u001b[0m | \u001b[0m 17.45   \u001b[0m | \u001b[0m 122.7   \u001b[0m |\n",
      "[00:13:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 2.374   \u001b[0m | \u001b[0m 5.044   \u001b[0m | \u001b[0m 233.5   \u001b[0m |\n",
      "[00:13:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 2.5     \u001b[0m | \u001b[0m 23.43   \u001b[0m | \u001b[0m 110.9   \u001b[0m |\n",
      "[00:13:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 2.464   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 170.1   \u001b[0m |\n",
      "[00:13:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m 2.5     \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 110.5   \u001b[0m |\n",
      "[00:13:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 2.439   \u001b[0m | \u001b[0m 16.93   \u001b[0m | \u001b[0m 142.8   \u001b[0m |\n",
      "[00:13:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 2.5     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 103.3   \u001b[0m |\n",
      "=================================================\n",
      "[00:13:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:13:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:13:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:13:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "|   iter    |  target   | max_depth | n_esti... |\n",
      "-------------------------------------------------\n",
      "[00:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 6.115   \u001b[0m | \u001b[0m 15.43   \u001b[0m | \u001b[0m 218.9   \u001b[0m |\n",
      "[00:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 5.92    \u001b[0m | \u001b[0m 5.003   \u001b[0m | \u001b[0m 97.68   \u001b[0m |\n",
      "[00:13:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 5.241   \u001b[0m | \u001b[0m 8.669   \u001b[0m | \u001b[0m 36.78   \u001b[0m |\n",
      "[00:13:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 5.922   \u001b[0m | \u001b[0m 9.657   \u001b[0m | \u001b[0m 110.2   \u001b[0m |\n",
      "[00:13:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 6.077   \u001b[0m | \u001b[0m 14.92   \u001b[0m | \u001b[0m 166.3   \u001b[0m |\n",
      "[00:13:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 6.108   \u001b[0m | \u001b[0m 15.55   \u001b[0m | \u001b[0m 220.3   \u001b[0m |\n",
      "[00:13:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 6.151   \u001b[0m | \u001b[95m 6.766   \u001b[0m | \u001b[95m 299.8   \u001b[0m |\n",
      "[00:13:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 6.138   \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 280.6   \u001b[0m |\n",
      "[00:13:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 6.11    \u001b[0m | \u001b[0m 30.0    \u001b[0m | \u001b[0m 193.4   \u001b[0m |\n",
      "[00:13:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 6.14    \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 268.7   \u001b[0m |\n",
      "[00:13:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 6.085   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 191.7   \u001b[0m |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (29.974713604713614, 299.3153731593596)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-14e20b5dc0f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 random_state=1,)\n\u001b[1;32m---> 76\u001b[1;33m     optimizer.maximize(init_points=5,  #执行随机搜索的步数\n\u001b[0m\u001b[0;32m     77\u001b[0m                         n_iter=30,)   #执行贝叶斯优化的步数\n\u001b[0;32m     78\u001b[0m \u001b[1;31m#     bayes_r2 = optimizer.max[\"target\"]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-10a58139642b>\u001b[0m in \u001b[0;36mblack_box_function\u001b[1;34m(n_estimators, max_depth)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mblack_box_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# min_samples_split, max_features,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     xg_bayes = XGBClassifier(n_estimators=int(n_estimators),\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#                                 min_samples_split=int(min_samples_split),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#                                 max_features=min(max_features, 0.999),  # float\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                 \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m         \u001b[0mevals_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1093\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0m_is_cudf_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_is_cudf_ser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1094\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mcupy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcp\u001b[0m  \u001b[1;31m# pylint: disable=E0401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\xgboost\\data.py\u001b[0m in \u001b[0;36m_is_cudf_df\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_is_cudf_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m         \u001b[1;32mimport\u001b[0m \u001b[0mcudf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 未调参\n",
    "roc_auc_value = 0\n",
    "accuracy_value = 0\n",
    "f1_value = 0\n",
    "recall_value = 0\n",
    "precision_value = 0\n",
    "#网格\n",
    "# wg_r2_value = 0\n",
    "# wg_mape_value = 0\n",
    "# wg_rmse_value = 0\n",
    "# wg_mae_value = 0\n",
    "\n",
    "# 贝叶斯\n",
    "bayes_roc_auc_value = 0\n",
    "bayes_accuracy_value = 0\n",
    "bayes_f1_value = 0\n",
    "bayes_recall_value = 0\n",
    "bayes_precision_value = 0\n",
    "\n",
    "#粒子群\n",
    "particle_roc_auc_value = 0\n",
    "particle_accuracy_value = 0\n",
    "particle_f1_value = 0\n",
    "particle_recall_value = 0\n",
    "particle_precision_value = 0\n",
    "\n",
    "xg = XGBClassifier(use_label_encoder=False)\n",
    "\n",
    "for i in range(50):\n",
    "    # 未调参\n",
    "    np.random.seed(i)\n",
    "    random_100 = get_random_more(100)\n",
    "    X = random_100.iloc[:,:-1]\n",
    "    y = random_100.iloc[:,-1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "    \n",
    "    xg.fit(X_train, y_train)\n",
    "    y_predict = xg.predict(X_test)\n",
    "    roc_auc_score_ = roc_auc_score(y_test,y_predict)\n",
    "    accuracy_score_ = accuracy_score(y_test,y_predict)\n",
    "    f1_score_ = f1_score(y_test, y_predict)\n",
    "    recall_score_ = recall_score(y_test, y_predict)\n",
    "    precision_score_ = precision_score(y_test, y_predict)\n",
    "    \n",
    "    roc_auc_value += roc_auc_score_\n",
    "    accuracy_value += accuracy_score_\n",
    "    f1_value += f1_score_\n",
    "    recall_value += recall_score_\n",
    "    precision_value += precision_score_\n",
    "    \n",
    "    # 网格\n",
    "#     parameters_range = dict(n_estimators=[i for i in range(10, 300, 10)],\n",
    "#                             max_depth=[x for x in range(5,31)])\n",
    "#     gs = GridSearchCV(xg, parameters_range,cv=5,verbose=2,n_jobs=-1)\n",
    "#     search = gs.fit(X_train, y_train)\n",
    "#     xg_opt = search.best_estimator_\n",
    "#     xg_opt.fit(X_train, y_train)\n",
    "#     xg_y_predict = xg_opt.predict(X_test)\n",
    "    \n",
    "#     wg_r2_score = metrics.r2_score(y_test,xg_y_predict)\n",
    "#     wg_mape_score = mape(y_test,xg_y_predict)\n",
    "#     wg_rmse_score = np.sqrt(metrics.mean_squared_error(y_test, xg_y_predict))\n",
    "#     wg_mae_score = metrics.mean_absolute_error(y_test, xg_y_predict)\n",
    "#     wg_r2_value += wg_r2_score\n",
    "#     wg_mape_value += wg_mape_score\n",
    "#     wg_rmse_value += wg_rmse_score\n",
    "#     wg_mae_value += wg_mae_score\n",
    "    \n",
    "   \n",
    "    # 贝叶斯\n",
    "    optimizer = BayesianOptimization(\n",
    "                f=black_box_function,\n",
    "                pbounds=pbounds,\n",
    "                verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "                random_state=1,)\n",
    "    optimizer.maximize(init_points=5,  #执行随机搜索的步数\n",
    "                        n_iter=30,)   #执行贝叶斯优化的步数\n",
    "#     bayes_r2 = optimizer.max[\"target\"]\n",
    "    bayes_max_depth = optimizer.max[\"params\"][\"max_depth\"]\n",
    "    bayes_n_estimators = optimizer.max[\"params\"][\"n_estimators\"]\n",
    "    bayes_model = XGBClassifier(max_depth = int(bayes_max_depth), n_estimators = int(bayes_n_estimators),\n",
    "                                use_label_encoder=False)\n",
    "    bayes_model.fit(X_train, y_train)\n",
    "    xg_bayes_predict = bayes_model.predict(X_test)\n",
    "    \n",
    "    bayes_roc_auc_score_ = roc_auc_score(y_test,xg_bayes_predict)\n",
    "    bayes_accuracy_score_ = accuracy_score(y_test,xg_bayes_predict)\n",
    "    bayes_f1_score_ = f1_score(y_test, xg_bayes_predict)\n",
    "    bayes_recall_score_ = recall_score(y_test, xg_bayes_predict)\n",
    "    bayes_precision_score_ = precision_score(y_test, xg_bayes_predict)\n",
    "    \n",
    "    bayes_roc_auc_value += bayes_roc_auc_score_\n",
    "    bayes_accuracy_value += bayes_accuracy_score_\n",
    "    bayes_f1_value += bayes_f1_score_\n",
    "    bayes_recall_value += bayes_recall_score_\n",
    "    bayes_precision_value += bayes_precision_score_\n",
    "    \n",
    "      \n",
    "    # 粒子群\n",
    "    \n",
    "    optimal_rbf_pars, info, _ = optunity.maximize(black_box_function,\n",
    "                                                      num_evals=20,\n",
    "                                                      solver_name='particle swarm',\n",
    "                                                      n_estimators=[10,300],\n",
    "        #                                               min_samples_split=[2,25],\n",
    "        #                                               max_features = [0.1, 0.999],\n",
    "                                                      max_depth =  [5, 30]) # default: 'particle swarm'\n",
    "\n",
    "    #     print(optimal_rbf_pars)\n",
    "\n",
    "        # print(info)\n",
    "        # print(_)\n",
    "    particle_n_estimators = optimal_rbf_pars[\"n_estimators\"]\n",
    "    particle_max_depth = optimal_rbf_pars[\"max_depth\"]\n",
    "    particle_model = XGBClassifier(n_estimators = int(particle_n_estimators), max_depth = int(particle_max_depth),\n",
    "                                   use_label_encoder=False)\n",
    "    particle_model.fit(X_train,y_train)\n",
    "    xg_particle_predict = particle_model.predict(X_test)\n",
    "    \n",
    "    particle_roc_auc_score_ = roc_auc_score(y_test,xg_particle_predict)\n",
    "    particle_accuracy_score_ = accuracy_score(y_test,xg_particle_predict)\n",
    "    particle_f1_score_ = f1_score(y_test, xg_particle_predict)\n",
    "    particle_recall_score_ = recall_score(y_test, xg_particle_predict)\n",
    "    particle_precision_score_ = precision_score(y_test, xg_particle_predict)\n",
    "                                             \n",
    "                                             \n",
    "#     particle_r2_score = info.optimum\n",
    "    # df = optunity.call_log2dataframe(info.call_log)\n",
    "    # print(df.sort_values('value', ascending=False)[:10])\n",
    "    particle_roc_auc_value += particle_roc_auc_score_\n",
    "    particle_accuracy_value += particle_accuracy_score_\n",
    "    particle_f1_value += particle_f1_score_\n",
    "    particle_recall_value += particle_recall_score_\n",
    "    particle_precision_value += particle_precision_score_\n",
    "    \n",
    "    \n",
    "un_roc_auc= roc_auc_value / 50\n",
    "un_accuracy = accuracy_value / 50\n",
    "un_f1 = f1_value / 50\n",
    "un_recall = recall_value / 50\n",
    "un_precision = precision_value / 50\n",
    "\n",
    "# wg_r2= wg_r2_value / 50\n",
    "# wg_mape = wg_mape_value / 50\n",
    "# wg_rmse = wg_rmse_value / 50\n",
    "# wg_mae = wg_mae_value / 50\n",
    "\n",
    "bayes_roc_auc= bayes_roc_auc_value / 50\n",
    "bayes_accuracy = bayes_accuracy_value / 50\n",
    "bayes_f1 = bayes_f1_value / 50\n",
    "bayes_recall = bayes_recall_value / 50\n",
    "bayes_precision = bayes_precision_value / 50\n",
    "\n",
    "particle_roc_auc= particle_roc_auc_value / 50\n",
    "particle_accuracy = particle_accuracy_value / 50\n",
    "particle_f1 = particle_f1_value / 50\n",
    "particle_recall = particle_recall_value / 50\n",
    "particle_precision = particle_precision_value / 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_100 = pd.DataFrame([[un_roc_auc, un_accuracy, un_f1, un_recall,un_precision],\n",
    "             [bayes_roc_auc,bayes_accuracy,bayes_f1,bayes_recall,bayes_precision],\n",
    "             [particle_roc_auc,particle_accuracy,particle_f1,particle_recall,particle_precision],\n",
    "             [0,0,0,0,0]],\n",
    "             columns=[\"roc_auc\",\"accuracy\",\"f1\",\"recall\",\"precision\"],index = [\"未调参\",\"贝叶斯优化\",\"粒子群\",\"遗传算法\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823fc4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81774175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
